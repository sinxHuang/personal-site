---
title: "💭 当我们谈论LLM时，我们在谈论什么"
outline: deep
desc: "LLM基础与发展"
tags: "AI基础"
updateTime: "2025-10-21"
---
# 当我们谈论LLM时，我们在谈论什么
笔记基于上交IWIN FINS实验室的组会分享，浅过一点LLM的基础与发展。

🔗链接：https://www.bilibili.com/video/BV1VLnNzgETR/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=d928ed1cb25f3e43e4adc9d729829a32
## 为什么采用Transformer

> Attention是矩阵到矩阵到一个映射。
> LLM也是一个**神经网络**。

优势：
* 方便进行大规模并行计算（每一个token都跟其他全部输入相连接，不取决于其他token的计算）
* 每层不改变输入输出纬度
* 丰富的可学习参数
---

## LLM结构：以Qwen为例

### Qwen2.5-7B
```python
QwenForCausalLM(
    model=Qwen2Model(
        embed_tokens=Embedding(152064, 3584),
        layers=ModuleList(
            [Qwen2DecoderLayer(
                self_attn=Qwen2Attention(
                    q_proj=Linear(in_features=3584, out_features=3584, bias=True),
                    k_proj=Linear(in_features=3584, out_features=512, bias=True),
                    v_proj=Linear(in_features=3584, out_features=512, bias=True),
                    o_proj=Linear(in_features=3584, out_features=3584, bias=False),
                    rotary_emb=Qwen2RotaryEmbedding(),
                ),
                mlp=Qwen2MLP(
                    gate_proj=Linear(in_features=3584, out_features=18944, bias=False),
                    up_proj=Linear(in_features=3584, out_features=18944, bias=False),
                    down_proj=Linear(in_features=18944, out_features=3584, bias=False),
                    act_fn=SiLU(),
                ),
                input_layernorm=Qwen2RMSNorm(),
                post_attention_layernorm=Qwen2RMSNorm(),
            ) for _ in range(28)]
        ),
        norm=Qwen2RMSNorm(),
    ),
    lm_head=Linear(in_features=3584, out_features=152064, bias=False),
)
```
#### 总分总结构

1. embedding层：将输入token向量乘以矩阵，转换为矩阵
2. 经过很多轮堆叠的decoder（attention + 全连接层MLP）
3. 最后经过一个MLP，**输出是一个矩阵**（三维张量 [batch_size, seq_len, vocab_size] ）
#### 具体过程
1. 每一个字对应一个token - ID，经过embedding层映射为一行向量，则100字变成100 * 3584的矩阵
2. 每一个token分别进行attention计算：
	1. 具体而言，每一个token被复制成三份乘上一个矩阵（3584 * 128 * 28）变成QKV，进行attention计算，得到128 * 100的结果；
	2. 28为注意力头数，则共有28个128* 100的矩阵
	3. 将这些矩阵横向堆叠，得到（128 * 28 = 3584）* 100的结果
	4. 结果乘以矩阵Wout（3584 * 3584）得到最后的结果3584 * 100
3. 全线性连接层MLP层将输入先扩大为高维空间便于提取特征，再压缩回3584 * 100
4. 将 attention + MLP 重复28次
5. 通过 lm.head 得到152064（词表大小，内容为logits值，即下一个词的概率） * 100的矩阵
6. 通过 loss function 进行训练
7. 通过第100行的输入进行计算，得到第101行词的概率分布
---

## LLM使用与名词解释

### 大模型加载与运行流程（Hugging Face Transformers）
#### 模型文件结构示例（以 Qwen2.5-7B 为例）

```
Qwen2.5-7B/
├── .cache/
├── .gitattributes
├── LICENSE
├── README.md
├── config.json
├── generation_config.json
├── model.safetensors.index.json
├── tokenizer_config.json
├── tokenizer.json
├── vocab.json
├── merges.txt
├── model-00001-of-00004.safetensors
├── model-00002-of-00004.safetensors
├── model-00003-of-00004.safetensors
└── model-00004-of-00004.safetensors
```
#### ⚙️ 模型加载运行流程
以 Hugging Face Transformers 框架为例：

- **文本 → 分词器 + tokenizer → Token → Medel → Token → Tokenizer → 文本**
1. **读取** **config.json**
    - 确定模型结构（如层数、隐藏维度、注意力头数等）。
2. **构建分词器**
    - 使用 tokenizer_config.json 和 tokenizer.json 来加载词表与规则。
3. **读取索引文件**
    - 通过 model.safetensors.index.json 确定各权重文件（分片）的路径。
4. **加载权重文件**
    - 根据索引按需加载多个
        model-xxxxx-of-xxxxx.safetensors
        文件中的权重，并填充至模型结构中。
5. **读取** **generation_config.json**
    - 提供推理时的默认生成参数（如温度、top_p、max_length 等）。
##### config.json文件：
```json
{
  "architectures": [
    "Qwen2ForCausalLM" //模型类型：因果语言模型
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151643,
  "hidden_act": "silu", //激活函数
  "hidden_size": 3584, 
  "initializer_range": 0.02, //权重初始化使用截断高斯分布，标准差为0.02
  "intermediate_size": 18944,
  "max_position_embeddings": 131072, 
  "max_window_layers": 28, //滑动窗口，每个token只关注到窗口内的注意力
  "model_type": "qwen2",
  "num_attention_heads": 28,
  "num_hidden_layers": 28,
  "num_key_value_heads": 4,
  "rms_norm_eps": 1e-06, //RMS归一化：大部分LLM采用，用均方根进行缩放，不破坏位置信息
  "rope_theta": 1000000.0, //旋转位置编码（现在常用），不改变向量长度；此外还有绝对位置编码，直接加上位置编码向量，缺点是需要模型去学习位置信息
  "sliding_window": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16", 
  "transformers_version": "4.40.1",
  "use_cache": true, //存储已经计算过的attention值
  "use_mrope": false, 
  "use_sliding_window": false,
  "vocab_size": 152064
}
```

| **类型**            | **模型架构**        | **代表模型**         | **核心特点**                        |
| ----------------- | --------------- | ---------------- | ------------------------------- |
| 因果语言模型（Causal LM） | Decoder-only    | GPT, Qwen, LLaMA | 单向注意力：当前词仅依赖左侧上下文               |
| 掩码语言模型（Masked LM） | Encoder-only    | BERT, RoBERTa    | 双向注意力：通过 [MASK] 预测被遮盖的词         |
| 前缀语言模型（Prefix LM） | Decoder-only    | UniLM, GLM       | 利用 [MASK] 实现输入部分双向编码 + 输出部分单向解码 |
| 序列到序列模型（Seq2Seq）  | Encoder-Decoder | T5, BART         | 编码器-解码器结构，双向编码 + 单向解码           |
##### 温度参数与文本生成策略（Temperature Sampling）
$$
P(w_i) = \frac{ \exp(z_i / T) }{ \sum_j \exp(z_j / T) }
$$
- **T（Temperature）** 表示采样时对概率分布平滑程度的控制系数。
- **低温度（如 0.1 ~ 0.5）**  
	  - 放大高概率词的权重  
	  - 生成更**保守、确定性强**的文本  
	  - 常用于问答、摘要等对一致性要求高的任务
- **高温度（如 0.8 ~ 1.5）**  
	  - 平滑概率分布  
	  - 增加低概率词被选中的机会  
	  - 生成更**多样、有创意**的文本  
	  - 常用于故事生成、开放式对话等任务
##### 文本生成策略示例
```python
# 贪心搜索（Greedy Search）
model.generate(input_ids, max_length=50, num_beams=1, do_sample=False)
# 波束搜索（Beam Search, 波束宽度5）
model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True, do_sample=False)
# Top-K 采样（仅保留前 k 个高概率词）
model.generate(input_ids, max_length=50, do_sample=True, top_k=50)
# Top-P 采样（核采样, 只保留累计概率 p 的词）
model.generate(input_ids, max_length=50, do_sample=True, top_p=0.95)
# 组合策略（Top-P + 温度调节）
model.generate(input_ids, max_length=50, do_sample=True, top_p=0.95, temperature=0.7)
```
- 结论：
	- 通常 **Top-p + Temperature** 联合使用效果最佳。
	- 温度越高 → 生成越随机；温度越低 → 越确定。
	- num_beams 用于控制 Beam Search 的宽度；top_k 与 top_p 控制采样范围。
---

## LLM训练微调

### 将大模型用到垂直领域的四种方法
| 方法类别               | 是否修改模型参数 | 成本    | 灵活性     | 典型应用        |
| ------------------ | -------- | ----- | ------- | ----------- |
| 重新训练               | ✅ 是      | 💸 极高 | ⭐ 高     | 基础大模型训练     |
| 微调                 | ✅ 是（部分）  | 💰 中等 | ⭐⭐ 较高   | 专业领域适配      |
| RAG                | ❌ 否      | 💡 低  | ⭐⭐⭐ 高   | 企业知识问答、检索辅助 |
| Prompt Engineering | ❌ 否      | ⚡ 极低  | ⭐⭐⭐⭐ 最高 | 快速实验与定制化场景  |
#### 💰 重新训练（Re-training）
> **Money is all you need.**
#### ⚙️ 微调（Fine-tuning）
- **监督学习微调（Supervised Fine-Tuning）**
	- **Full Fine-Tuning**：全参数微调，更新所有权重，成本高但效果最好  
	- **LoRA（Low-Rank Adaptation）**：低秩适配，仅训练部分参数层，显著节省资源  
	- **Rejection Sampling Fine-Tuning**：基于拒绝采样的强化微调  
- **强化学习对齐（RLHF）**  
	* **GRPO**（Group Relative Preference Optimization）  
	* **DPO / KTO**（Direct Preference Optimization / Kullback–Leibler Preference Optimization）
#### 📚 检索增强生成（RAG, Retrieval-Augmented Generation）
> **本质是：搜索 + LLM**
1. 搜索模型在文档中查找与原始 Query 最相关的内容  
2. 将检索结果拼接进 Prompt  
3. 再输入给 LLM 生成最终答案  
#### 提示工程（Prompt Engineering）
> **设计合适的 Prompt，让模型更好地理解任务。**
### 监督学习微调
#### SFT
**SFT（Supervised Fine-Tuning）** 是大模型在预训练之后的 **第一阶段微调方式**。  

它通过**人工标注的高质量指令数据**，让模型学会遵循人类指令、输出符合预期格式和语义的结果。
简单来说：  
> 预训练让模型“会说话”，  
> SFT 让模型“听得懂话”。
  
1. **准备数据集**
   - 数据形式通常为指令-回答对（Instruction-Response pairs）
   - 例如：

   ```json
   {
     "instruction": "请解释光合作用的原理。",
     "input": "",
     "output": "光合作用是植物利用光能将二氧化碳和水合成为有机物的过程。"
   }
   ```
2. **加载预训练模型**
3. **构建训练数据**
    - 将 instruction + input 拼接成 prompt
    - 使用 tokenizer 进行编码
    - 对 output 计算 loss（监督信号），反向传播更新权重

| **方法**                    | **是否更新所有参数** | **资源开销** | **数据需求** | **优点**       | **适用场景**    |
| ------------------------- | ------------ | -------- | -------- | ------------ | ----------- |
| **Full Fine-Tuning**      | ✅ 是          | 💸 高     | 📦 大     | 改动彻底，性能强     | 小模型或高定制化任务  |
| **LoRA**                  | ⚙️ 仅部分层      | 💡 低     | 📦 中     | 参数高效，可快速切换任务 | 资源受限场景      |
| **Rejection Sampling FT** | ✅ 是（基于筛选数据）  | 💰 中等    | 📋 高质量数据 | 输出更安全可靠      | 安全对齐、内容生成优化 |
#### Full Fine-Tuning
**概念：**  
- 对模型的所有参数进行重新训练（包括 Transformer 层、嵌入层、LayerNorm 等）。  
- 可以选择性地“冻结”部分层（如底层 embedding 或 attention 层），仅更新上层权重以减少计算量。  

**特点：**
- ✅ 适合参数较小的模型或需要显著改变模型行为的场景  
- ⚠️ 计算和显存开销大，微调后的模型不易迁移  
- 🔧 常用于企业定制模型或科研项目
#### LoRA
**核心思想：**
> 将原始大矩阵 $W \in \mathbb{R}^{d \times k}$ 分解为两个低秩矩阵
> $A \in \mathbb{R}^{d \times r}$、$B \in \mathbb{R}^{r \times k}$，其中 $r \ll \min(d, k)$。

即：
$$

W’ = W + \Delta W = W + A \times B

$$
**特点：**
- ✅ 仅训练 LoRA 层参数（$A$、$B$），冻结原模型
- ✅ 显著降低显存与算力开销
- ✅ 可与多个任务共享同一基座模型
- ⚠️ 对复杂任务的表达能力略受限制
#### Rejection Sampling Fine-Tuning
**核心思想：**
- 通过人类反馈或规则过滤，**移除模型生成的不理想样本**，只保留优质样本进行 SFT。
- 本质是 **数据质量增强版的监督微调**。

**步骤：**
1. 模型生成多个候选回答。
2. 人工或算法打分，筛掉不符合预期的输出。
3. 将高质量样本重新组织为新的 SFT 数据集。
4. 重新执行监督式微调。

**特点：**
- ✅ 提升模型输出质量与安全性
- ✅ 不需要复杂的奖励模型（相比 RLHF 更简单）
- ⚠️ 成本高（需人工筛选）
### 强化学习对齐
强化学习对齐（RLHF）是让大语言模型“符合人类偏好”的关键阶段。  

其核心流程为：
1. **SFT 模型（监督微调模型）** 作为初始策略模型（Policy Model）  
2. **Reward Model（奖励模型）** 学习人类偏好  
3. **Policy Model** 通过强化学习算法（如 PPO / GRPO）优化，使输出更符合奖励模型偏好
#### PPO
**思想：**  

通过“剪切函数 (clip)” 防止策略更新过大，稳定训练。
$$
J_{\text{PPO}}(\theta) =
\mathbb{E}\!\left[
  \min\!\left(
    \frac{\pi_\theta(a \mid s)}{\pi_{\text{old}}(a \mid s)} A(s,a),\;
    \text{clip}\!\left(
      \frac{\pi_\theta(a \mid s)}{\pi_{\text{old}}(a \mid s)},
      1 - \epsilon,\;
      1 + \epsilon
    \right) A(s,a)
  \right)
\right]
$$
- $\pi_\theta(a|s)$：当前策略模型的动作概率  
- $\pi_{old}(a|s)$：旧策略（上一次更新前的模型）  
- $A(s,a)$：优势函数（Advantage Function）  
- $\epsilon$：限制更新幅度的超参数  

**结构组成：**
- **Policy Model**：待优化的大语言模型 ，即SFT模型
- **Reward Model**：计算每个输出的得分（基于人类偏好），通过“输入-输出对”得到数值打分，本质监督学习model
- **Value Model**（需要训练，被GPPO优化了）：预测期望总奖励，用于估计优势函数 $A(s,a)=r(s,a)-V(s)$ （代表「当前动作比预期好多少」）
- **Reference Model**：提供 KL 约束（衡量两个分布的差异性），防止偏离原语义分布；通常是 SFT 模型的冻结副本
#### GRPO
**提出原因：**  

传统 PPO 对单样本更新，计算不稳定。  

**GRPO** 通过 **组内相对比较（Group-based Ranking）** 提高稳定性与样本利用率。  
$$
J_{\text{GRPO}}(\theta) =
\mathbb{E}\!\left[
  \frac{1}{G}\sum_{i=1}^{G}
  \min\!\left(
    \frac{\pi_\theta(a_i \mid s)}{\pi_{\text{old}}(a_i \mid s)} \lambda_i,\;
    \text{clip}\!\left(
      \frac{\pi_\theta(a_i \mid s)}{\pi_{\text{old}}(a_i \mid s)},
      1 - \epsilon,\;
      1 + \epsilon
    \right)\lambda_i
  \right)
\right]
$$
- $G$：每组候选输出的数量
- $\lambda_i$：样本的相对得分（由 Reward Model 计算）
- 通过组内比较来计算梯度，使训练更稳定

**结构组成：**
- **Policy Model**：多运行几次，输出多个候选结果 $\{o_1, o_2, ..., o_G\}$  
- **Reward Model**：为每个输出计算奖励 $r_i$  
- **Group Computation**：根据组内得分差异计算 $\lambda_i$，用组内相对好坏来转化成权重替代原来的Value Model
- **Reference Model**：保持 KL 正则化，避免语言漂移
#### DPO/KTO：有监督学习的思想，学习偏好的隐式
**优化目标：**  

通过偏好数据（如人类对回答 A vs. B 的排序）隐式学习奖励函数，**直接优化偏好概率**，无需显式的强化学习过程。定义一个loss function梯度下降，类似于监督学习。
##### **💡 DPO 的损失函数（基于 Bradley–Terry 模型）**
$$
L_{\text{DPO}}
= -\mathbb{E}_{(x,y_w,y_l)\sim D}\,
\log \sigma\!\left(
  \beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{\text{ref}}(y_w \mid x)}
 -\beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{\text{ref}}(y_l \mid x)}
\right)
$$

其中：
- $y_w$：被人类评为“更好”的回答（winner）
- $y_l$：被评为“更差”的回答（loser）
- $\pi_\theta$：待优化模型
- $\pi_{\text{ref}}$：参考模型（通常是 SFT 模型）
- $\beta$：控制 KL 正则强度的超参数
##### 💡 KTO 的损失函数（基于前景理论）
KTO 在 DPO 基础上引入“**收益（selected answer）**”与“**损失（unselected answer）**”的非对称加权，让模型在偏好强化与惩罚间取得更灵活的平衡。
- **收益部分（Gain）**：放大“优选回答”的优化力度
- **损失部分（Loss）**：抑制“不优回答”的概率增长
> KTO 的具体数学形式根据实现略有差异，但总体可理解为带权的 DPO。

---

## 总结与思考

大模型泡沫带来了两种方向的应用，一方面叫做提效增速（比如AI coder、AI PS、Agent搜索），一方面叫做创造新的需求（比如AI游戏、AI Friends、具身智能）。对于我这种科研民工和未来打工人来说，两者都有必要去学习，分别对应了短期需求和长期需求。

当然现在我做的方向更偏向于大模型Efficiency那一块，关注于前期准备和基础，与应用还有一定差距。
>🤔 Other Thoughts：
> 大语言模型能够颠覆AI行业，我想可能一定原因也在于，语言能够承载人的灵魂；当我学会了母语之后，思维才有了具象化，以至于现在的我完全无法想象没有语言的时候人类如何思考。