---
title: "🧩 RoPE：旋转位置编码"
outline: deep
desc: "Transformer位置编码方法与改进"
tags: "AI基础"
updateTime: "2025-11-17"
---
# RoPE：旋转位置编码
## 我们为什么需要位置编码？
### 绝对位置编码
把词嵌入向量 $x_k$ 与其位置向量 $p_k$ 相加，其中 $p_k$ 只依赖于位置信息（位置 $k$）：
$$
x_k + p_k
$$
### 正余弦位置编码（Transformer）
$$
p_{k,2i} = \sin\left(\frac{k}{10000^{2i/d}}\right), \quad p_{k,2i+1} = \cos\left(\frac{k}{10000^{2i/d}}\right)
$$
其中，$p_{k,2i}$ 和 $p_{k,2i+1}$ 是位置 $k$ 的编码向量的第 $2i$ 和第 $2i+1$ 个分量，  
$d$ 是位置向量的维度，$k$ 表示第几个 token（位置）。

---
## 我们为什么需要 RoPE？
### 正余弦位置编码的局限
正余弦位置编码具备一定的表达**相对位置**的能力：
- token $i$ 的词嵌入向量为 $x_i$，位置向量为 $PE(i)$  
- token $j$ 的词嵌入向量为 $x_j$，位置向量为 $PE(j)$  
attention 得分的本质是计算 token $i$ 对 token $j$ 的关注度。  
当 $x_i$ 与 $x_j$ 做内积时，模型虽然能够间接捕捉到位置关系，  但需要额外“费力”从内积结果中分离出**位置差异**与**语义相关度**。
### 把位置信息乘上词嵌入向量
这就是 **RoPE（Rotary Positional Embedding）** 的思想。
设：
- 位置为 $m$ 的 token 为 $q_m$  
- 位置为 $n$ 的 token 为 $k_n$  
将词嵌入向量与绝对位置信息相乘：
$$
q_m e^{i m \theta}, \quad k_n e^{i n \theta}
$$
此时我们计算两个 token 之间的注意力：
$$
\langle q_m e^{i m \theta}, k_n e^{i n \theta} \rangle
= \operatorname{Re}\left[(q_m e^{i m \theta})(k_n e^{i n \theta})^*\right]
= \operatorname{Re}\left[q_m k_n^* e^{i (m-n) \theta}\right]
$$
因此，RoPE 通过旋转操作自然地将**相对位置信息**编码进了注意力计算中。
### 为什么叫旋转位置编码？
词嵌入向量与绝对位置信息相乘，相当于把向量进行旋转。  
因此这种方法被称为**旋转位置编码（Rotary Positional Embedding, RoPE）**。
### RoPE 的优点
- 将**相对位置信息**显式融入注意力得分计算；
- 具备**外推（extrapolation）** 的潜力，能处理超出训练长度的序列。
---
## 什么是外推？怎么外推？
### 外推的定义
外推是指解决**训练长度**与**预测长度**不一致的问题。
### 成因
1. 预测时会用到**未训练过的位置编码**；  
2. 预测时注意力机制要处理的 token 远超训练阶段的局部范围。
### 一个解决思路：“超强基线模型”
- 采用**局部 attention**；  
- 使用**掩码（masking）**，让预测时每个 token 只能看到训练长度范围内的 token。
#### 实际效果
这种方法在实践中效果有限，仍存在性能衰减问题。  
可能的原因：开头的几个token很重要，不能mask掉。

---
## 线性缩放（位置内插 Positional Interpolation）
将超长的位置按比例压缩，这一过程称为**位置内插**。  
它通过压缩邻近 token 的距离，使模型在较长序列中依然能处理位置编码。  
但这种方法会**严重扰乱模型的局部分辨率**。
* 外推：from  `1—1—1—1—1—1—1—1`  to  `1——1——1——1——1——1——1`
* 内插：from  `1——1——1——1——1——1——1`  to  `1-1-1-1-1-1-1-1`
## NTK-aware Scaled RoPE
### 思想
**低频内插，高频外推**  
将外推压力平均分摊到每一个维度上。
在 RoPE 中：
- 维度越高 → 频率越低 → 波长越长  
- 维度越低 → 频率越高 → 波长越短  
训练位置范围（高频 → 低频）：
$$
[0 \; ---- \; 511]_{\text{低频}} \quad (L_{\text{train}} = 512)
$$
推理位置范围（高频 → 低频）：
$$
[0 \; ----------- \; 2047]_{\text{低频}} \quad (L_{\text{infer}} = 2048)
$$
### 实现思路
- **低频维度**：波长极长，采用**内插**，即压缩全局位置信息；  
- **高频维度**：波长极短，采用**外推**，保留局部位置的精细差异。
示意：  
高频：`1—1—1—1—1—1—1—1—1—1—1—1` 低频    
高频：`1——1——1——1-1-1-1-1-1-1-1-1` 低频
### NTK-aware Scaled RoPE 存在的问题
当波长大于原最大序列长度时（即低频分量），  若对这些维度进行外推，会引入**模型从未见过的旋转角度**。  
这些旋转角度对应的正余弦值在训练过程中模型也未曾学习过，  从而导致外推性能下降。
### 解决办法
- 对于**波长大于训练长度（低频）** 的维度：仅进行**内插**；  
- 对于**波长很短（高频）** 的维度：进行**完全外推**；  
- 对于**中间频率**的维度：采用原始的 **NTK-aware Scaled RoPE**。
这种分区处理的改进方法被称为：**NTK-by-parts**。
---
## YaRN：Yet another RoPE extension method
**论文标题：**  
*YaRN: Efficient Context Window Extension of Large Language Models*  
**作者：**  
Bowen Peng¹, Jeffrey Quesnelle¹, Honglu Fan²³, Enrico Shippole  
¹Nous Research ²EleutherAI ³University of Geneva
### 方法核心
YaRN = **NTK-by-parts + attention-scaling**
它结合了 NTK-by-parts 的分频插值策略与注意力缩放机制，从而高效扩展大模型的上下文窗口。
### Leaky ReRoPE
#### 思想
只要窗口大小 $w$ 选定为**小于训练长度**，  
就可以通过控制参数 $k$，在保持局部性的前提下，  
让所有位置编码都不超过训练时的最大长度。
#### 方法原理
通过平滑过渡的方式，将**直接外推**与**位置内插**结合：
- 当位置小于 $w$ 时，使用标准编码；
- 当位置超过 $w$ 时，通过控制 $k$ 缓慢递增，使得外推角度逐步“泄漏”（Leaky）；
- 这种方法能在不破坏局部信息的情况下，扩展可用上下文长度。
示意图中红色部分表示标准编码区域（训练范围内），  绿色部分表示“泄漏外推”区域（超出训练长度后平滑延伸）。
![](/RoPE1.png)
### Leaky ReRoPE 的极限情况
当 $k \to \infty$ 时，  
**Leaky ReRoPE** 退化为 **ReRoPE（Recurrent RoPE）**。
此时，无论输入序列长度是多少，  其位置编码范围都被限制在 $[0, w]$ 之内，  即模型的位置信息始终保持在训练长度范围内。
#### 特点
- 位置编码不会继续增长，而是循环或复用已有编码；  
- 理论上可以支持**任意长度的上下文（Context）**；  
- 兼具稳定性与扩展性，避免外推引发的角度漂移问题。